---
title: "Drunk_Modeling_2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(lubridate)
library(keras)
library(tensorflow)
physical_devices <- tf$config$list_physical_devices('GPU')
tf$config$experimental$set_memory_growth(device = physical_devices[[1]], enable = T)
```

# Load Data

```{r}
load("Booze_Data.R")
```

# Build generator

```{r}
generator <- function(data, # Input data
                      flanking, # How many timepoints on each side to capture
                      delay=0, # time we want to predict on
                      min_index, # Earliest timepoint to use
                      max_index, # Latest timepoint to use
                      shuffle = TRUE, # If order of sampling matters, no in this case
                      batch_size = 32, # Samples per a batch
                      step = 1) { # How often we want to sample to sample data
  if (is.null(max_index))
    max_index <- nrow(data) - delay - flanking - 1
  i <- min_index + flanking
  function() {
    if (shuffle) {
      rows <- sample(c((min_index+flanking):(max_index-flanking)), size = batch_size)
    } else {
      if (i + batch_size >= max_index)
        i <<- min_index + flanking
      rows <- c(i:min(i+batch_size, max_index))
      i <<- i + length(rows)
    }
    
    samples <- array(0, dim = c(length(rows), 
                                flanking / step + 1,
                                3)) # only want x, y, and z
    targets <- array(0, dim = c(length(rows)))
                     
    for (j in 1:length(rows)) {
      num_pids <- length(unique(data[(rows[[j]]-flanking):rows[[j]],2]))
      while(num_pids != 1){
        rows[[j]] <- rows[[j]] + flanking
        num_pids <- length(unique(data[(rows[[j]]-flanking):rows[[j]],2]))
      }
      samples[j,,] <- as.matrix(data[(rows[[j]]-flanking):rows[[j]], c(3,4,5) ])
      targets[[j]] <- as.matrix(data[rows[[j]] + delay, 8])
    }            
    list(samples, targets)
  }
}
```

# Make generators
```{r}
flanking <- 300
step <- 1
delay <- 0
batch_size <- 64
train_gen <- generator(
  data,
  flanking = flanking,
  delay = delay,
  min_index = 1,
  max_index = NULL,
  shuffle = TRUE,
  step = step,
  batch_size = batch_size
)
val_gen = generator(
  data,
  flanking = flanking,
  delay = delay,
  min_index = 1,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
test_gen <- generator(
  data,
  flanking = flanking,
  delay = delay,
  min_index = 1,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
val_steps <- 100            
test_steps <- 100
```

# Test Model

```{r}
model <- keras_model_sequential() %>%
  layer_conv_1d(
    filters = 32,
    kernel_size = 5,
    activation = "relu",
    input_shape = list(NULL, 3)
  ) %>%
  layer_max_pooling_1d(pool_size = 3) %>%
  layer_conv_1d(filters = 32,
                kernel_size = 5,
                activation = "relu") %>%
  bidirectional(
    layer_gru(
      units = 32,
      return_sequences = TRUE,
      recurrent_activation = "sigmoid",
      use_bias = TRUE,
      reset_after = TRUE
    )
  ) %>%
  bidirectional(
    layer_gru(
      units = 64,
      recurrent_activation = "sigmoid",
      use_bias = TRUE,
      reset_after = TRUE
    )
  ) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- model %>% fit(
  train_gen,
  steps_per_epoch = 500,
  epochs = 20,
  validation_data = val_gen,
  validation_steps = val_steps
)
```


```{r}
plot(history)
```

```{r}
model %>% evaluate(test_gen,  steps = test_steps) # 88.94%
```


## RUN IT BIGGER

# Make generators
```{r}
flanking <- 300
step <- 1
delay <- 0
batch_size <- 256
train_gen <- generator(
  data,
  flanking = flanking,
  delay = delay,
  min_index = 1,
  max_index = NULL,
  shuffle = TRUE,
  step = step,
  batch_size = batch_size
)
val_gen = generator(
  data,
  flanking = flanking,
  delay = delay,
  min_index = 1,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
test_gen <- generator(
  data,
  flanking = flanking,
  delay = delay,
  min_index = 1,
  max_index = NULL,
  step = step,
  batch_size = batch_size
)
val_steps <- 1000            
test_steps <- 1000
```

# Test Model

```{r}
model <- keras_model_sequential() %>%
  layer_conv_1d(
    filters = 32,
    kernel_size = 5,
    activation = "relu",
    input_shape = list(NULL, 3)
  ) %>%
  layer_max_pooling_1d(pool_size = 3) %>%
  layer_conv_1d(filters = 32,
                kernel_size = 5,
                activation = "relu") %>%
  bidirectional(
    layer_gru(
      units = 32,
      return_sequences = TRUE,
      recurrent_activation = "sigmoid",
      use_bias = TRUE,
      reset_after = TRUE
    )
  ) %>%
  bidirectional(
    layer_gru(
      units = 64,
      recurrent_activation = "sigmoid",
      use_bias = TRUE,
      reset_after = TRUE
    )
  ) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = optimizer_rmsprop(),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

history <- model %>% fit(
  train_gen,
  steps_per_epoch = 500,
  epochs = 50,
  validation_data = val_gen,
  validation_steps = val_steps
)
```

```{r}
plot(history)
```

```{r}
model %>% evaluate(test_gen,  steps = test_steps) #97.88%
```